## Subjects
There are 20 subject datasets each with a pre (ses-pre) and (ses-post) scan, collected from consenting neurotypical subjects with standard inclusion/exclusion criteria (English speaking, right handed, no history of neurological or psychiatric illness) and the protocol was approved by the Carnegie Mellon University Institutional Review Board.

## Experimental Task
BOLD data were collected eyes open at rest looking at a black screen. Participants were taught to perform multi-finger movements that were recorded on a button glove (index, middle, ring, little) in response to visual stimuli (stimuli/*.png) that were projected onto the screen in groups of 4 element sets. Participants learned a unique mapping of cue to finger press prior to the each imaging session. Each stimulus corresponded to one of 4 unique key presses. Participants had 6 seconds to initiate the 4 finger response to the 4 cues that appeared on the screen. The two fMRI sessions, ses-pre and ses-post, were collected before and after a 10 day training regimen, where participants either practiced a 16 element movement sequence (n= 10) or a 16 element cue sequence (n=10 control) for approximately 30 minutes each day. The cue sequence was the same visually across days, while the movement sequence was the same motorically across days. Group identity and sex for each subject is available in /participants.tsv. Trial types in each of the events.tsv file correspond to sets of 4 key presses MS: Movement set, CS: Cue set. Additional details are available in the manuscript:
Patrick Beukema, JÃ¶rn Diedrichsen, Timothy Verstynen. 

## Data Acquisition
Data were acquired with a Siemens Verio 3T MRI Scanner and a 32-channel head coil. 8 separate runs for both the ses-pre and ses-post session were acquired. Each run consisted of acquiring 241 functional volumes (~8.03 min) of the same sets but in different orderings and following novel mappings between each run. All sequence protocol details for both the functional and anatomical images can be found in /sequence_protocols. Pydeface was used on all anatomical images to ensure de-identification of subjects. The code can be found at https://github.com/poldracklab/pydeface. 
